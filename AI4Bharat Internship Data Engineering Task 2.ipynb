{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing pdf_url from given page_url in the excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('C:/Users/aamir/urls.csv')\n",
    "df.head()\n",
    "df=df[['URLS']]\n",
    "df['pdf_url']=''  #Creating a column to store pdf url\n",
    "df['name']=''     #Creating a column to store name of pdf\n",
    "\n",
    "  \n",
    "def get_pdf_url_and_pdf_name(url):\n",
    "    if '.pdf' in url:                         #If url has .pdf then page and pdf url will be same\n",
    "        split_url = url.split('/')\n",
    "        filename = split_url[-1][:-4]+'.pdf'  #Splitting the url at every ' ' and extracting the name of the book\n",
    "        return url, filename\n",
    "    else:                                     #if .pdf is not in url then looking for it in the HTMl code\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.findAll('a',{'class':'format-summary download-pill'}) #looking for anchor tags with this particular class\n",
    "        for link in links:                                                 # to get the download link\n",
    "            if ('.pdf' in link.get('href', [])):\n",
    "                href = 'https://archive.org'+link.get('href')\n",
    "                split_url = href.split('/')\n",
    "                filename = split_url[-1][:-4]+'.pdf'\n",
    "                return href, filename\n",
    "\n",
    "for i in range(len(df)): #Note for 46th url given in the excel sheet the class attribute was different                                                                   \n",
    "    if i==46:            # Therefore I have skipped the 46th URl over here. I performed OCR separately for this book and appended it to the JSON\n",
    "        continue\n",
    "    else:\n",
    "        df.loc[i,'pdf_url'],df.loc[i,'name']=download_pdfs(df.loc[i,'URLS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading pdf's from the given urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs(url):       #Same as above but this function is used to download the pdf\n",
    "    if '.pdf' in url:\n",
    "        split_url = url.split('/')\n",
    "        filename = split_url[-1][:-4]+'.pdf'\n",
    "        response = requests.get(url)\n",
    "        pdf = open('C:/Users/aamir/Marathi_pdfs/'+filename, 'wb')\n",
    "        pdf.write(response.content)\n",
    "        pdf.close()\n",
    "        #return url, filename\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.findAll('a',{'class':'format-summary download-pill'}) \n",
    "        for link in links:\n",
    "            if ('.pdf' in link.get('href', [])):\n",
    "                href = 'https://archive.org'+link.get('href')\n",
    "                split_url = href.split('/')\n",
    "                filename = split_url[-1][:-4]+'.pdf'\n",
    "                response = requests.get(href)\n",
    "                pdf = open('C:/Users/aamir/Marathi_pdfs/'+filename, 'wb')\n",
    "                pdf.write(response.content)\n",
    "                pdf.close()\n",
    "                #return href, filename\n",
    "                \n",
    "df['URLS'].apply(download_pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting PDF's to IMG and applying OCR using Tesseract. Storing the results as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from pytesseract import image_to_string  #Used for OCR \n",
    "import sys\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def convert_pdf_to_img(pdf_file):   #First step is to convert pdf to image format\n",
    "   \n",
    "    return convert_from_path(pdf_file,poppler_path='C:/Users/aamir/Downloads/poppler-0.68.0/bin')\n",
    "\n",
    "\n",
    "def convert_image_to_text(file): #Using to tesseract to apply OCR to convert image to text.\n",
    "    \n",
    "   \n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe'\n",
    "    text = image_to_string(file, lang='mar') #language is marathi\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text_from_any_pdf(path,page_url,pdf_url): #This function takes in the pdf path, page_url and pdf_url\n",
    "    \n",
    "    images = convert_pdf_to_img(path)\n",
    "    final_text = \"\"\n",
    "    book_json={}\n",
    "    result={}\n",
    "    for pg, img in enumerate(images):\n",
    "        \n",
    "        final_text = convert_image_to_text(img)   #For each page we get the extracted text\n",
    "        #print(\"Page nÂ°{}\".format(pg))\n",
    "        #print(convert_image_to_text(img))\n",
    "        book_json[pg]=final_text                #Created a dictionary with pg no. as key and extracted text as value\n",
    "    result['page_url'] = page_url               #The resultant dictionary contains the required output in the specified format\n",
    "    result['pdf_url'] = pdf_url\n",
    "    result['paragraph'] = book_json             #The paragraph contains entire content of pdf with each page and its text\n",
    "    return result\n",
    "\n",
    "json_result=[]\n",
    "for x in range(len(df)):\n",
    "    print(x)\n",
    "    if i==46:                 #Note for 46th url given in the excel sheet the class attribute was different\n",
    "        continue              # Therefore I have skipped the 46th URl over here. I performed OCR separately for this book and appended it to the JSON\n",
    "    else:\n",
    "        filename = df['name'][x]\n",
    "        json_result.append(get_text_from_any_pdf('C:/Users/aamir/Marathi_pdfs/'+filename,df['URLS'][x],df['pdf_url'][x]))\n",
    "\n",
    "\n",
    "with open('pdf_extract.json', 'w', encoding='utf8') as file: #Saving as json file\n",
    "    file.write(str(json_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
